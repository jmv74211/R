---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}

library(tidyverse)
library(funModeling)
library(caret)

data_raw <- read_csv('titanic/train.csv')
df_status(data_raw)
data_raw

```

# Preprocesamiento

Preprocesamiento sencillo de nuestros datos


```{r}

data <-
  data_raw %>%
  mutate(Survived = as.factor(ifelse(Survived == 1, 'Yes', 'No'))) %>%
  mutate(Pclass = as.factor(Pclass)) %>%
  mutate(Fare_Interval = as.factor(
    case_when(
      Fare >= 30 ~ 'More.than.30',
      Fare >= 20 & Fare < 30 ~ 'Between.20.30',
      Fare < 20 & Fare >= 10 ~ 'Between.10.20',
      Fare < 10 ~ 'Less.than.10'))) %>%
  select(Survived, Pclass, Sex, Fare_Interval)

```

# Particionamiento: entrenamiento y validación.

Se realiza el particionamiento en datos de entrenamiento y datos de validación.

La partición crea dos conjuntos de índices.
 - Índices de entrenamiento
 - Índices de validación
 
 trainIndex: Número de filas del conjunto de entrenamiento.
 
 Parámetros: p = proporción. En este caso 0.7 (70%)
 

```{r}
# Particionamiento: entrenamiento y validación.

set.seed(0)
trainIndex <- createDataPartition(data$Survived, p = .7, list = FALSE, times = 1)
train <- data[trainIndex, ] 
val   <- data[-trainIndex, ]

```


# Creación del modelo

Primero se va a crear una estructura de datos que es el resultado de `train_control` que es una estructura de datos donde se define cómo se quiere que se realice el proceso de aprendizaje.

El segundo parámetro es un grid que sirve para especificar una serie de parámetros en el algoritmo de aprendizaje.

cp (complexity parameter) es un valor que representa la profundidad máxima permitida que es el resultado de los árboles de aprendizaje. 0.01 es un valor muy típico. Caret nos permite darle un grid como una lista de valores ej: .cp = c(0.01,0.05,01) Esto probaría los modelos con los distintos parámetros y directamente se quedaría con el mejor.

- En el árbol de decisión solo tenemos un parámetro (cp).
- En el resto de algoritmos se pueden usar más parámetros. A caret le podemos decir que cree un grid con los valores de a = (0.1,0.7) y b = (5,7,8) y Caret va a probar con todos los valores posibles en dichas combinaciones de pares. 

A la izquierda tenemos la variable de predicción seguido de ~ y a continuación se especifica qué columnas se van a utilizar para la predicción. un "." significa todas.

```{r}
rpartCtrl <- trainControl(classProbs = TRUE)
rpartParametersGrid <- expand.grid(.cp = c(0.01))

rpartModel <- train(Survived ~ .,
                    data = train,
                    method = "rpart",
                    metric = "Accuracy",
                    trControl = rpartCtrl,
                    tuneGrid = rpartParametersGrid )
```


Visualización del modelo de predicción.

Se puede utilizar 

- library(partykit)
- library(rattle): Visualización del modelo. Es posible en en linux de problemas por la necesidad de un programa ¿quartz?


En el siguiente código se transforma el modelo

IMportante: Puede darnos un fallo si no hemos definido correctamente el nombre de las etiquetas de las variables.

Rpart nos genera un árbol de clasificación. Se puede controlar la granuralidad de dicho árbol. Como salida obtenemos la probabilidad de que un individuo pertenezca a dicha clase.

```{r}
library(partykit)
library(rattle)
rpartModel_party <- as.party(rpartModel$finalModel)
plot(rpartModel_party)
fancyRpartPlot(rpartModel$finalModel)

asRules(rpartModel$finalModel) # Nos da las reglas de decisión que se se van a utilizar en nuestro modelo. Son los caminos del árbol en forma de reglas.
```

# Predicción del modelo

Ahora vamos a aplicar dicho modelo con el conjunto de test. Para ello vamos a utilizar la función llamada `predict`. Se le pasa el conjunto de datos y el modelo y nos devuelve

```{r}
prediction <- predict(rpartModel,val,type ="raw") # raw o probabilites. Raw se puede obtener a través de si, si: prob >= 0.5 y no si prob < 5
cm_train <- confusionMatrix(prediction,val[["Survived"]]) # Matriz de recuento de para cuántos se ha acertado y cuántos no. Concept básico de matriz de confusión
cm_train

```

```{r}
plotdata <- val %>%
  cbind(prediction)
ggplot(data = plotdata) +
  geom_histogram(aes(x = Survived, fill = prediction), stat = "count")
```

# Medidas de la clasificación automática

Precision nos mide el porcentaje de aciertos que hemos tenido.
Sensitivity/recall es el número de True positive respecto al número de positive en el número de casos reales. Número de aciertos.

FScore: Es un agregado de precision y recall. Es muy típico de los problemas de recuperación de información. A mejor Fscore, mejor será nuestro modelo.

## Curva ROC

Suponemos que tenemos los ejemplos ordenados por la clase positiva. Una vez que está ordenado, si un ejemplo va pertenece a la clase positiva vamos hacia arriba, y si no vamos a la derecha.

Eje x es el total de ejemplos negativos
Eje y es el total de ejemplos positivos.

El clasificador perfecto sería uno que tiene la forma | -

El peor clasificador sería uno que tiene una forma _|

Un clasificador aleatorio tiene la forma de la ecuación y = x

El acurracy puede tener problemas para medir la importancia entre falsos positivos y falsos negativos. Para ello vamos a utilizar la curva ROC. En R es muy sencillo calcular todos los parámetros necesarios. Podemos decir a nuestro modelo que queremos utilizar el valor AUC.


```{r}
# twoClassSumary es necesario ponerlo sí o si cuando se quiere utilizar una curva ROC.
# Creamos el modelo utilizando la métrica de ROC. Si el aprendizaje se le indica una métrica, el modelo se queda con el mejor resultado obtenido.

# Ahora probamos con probabilidad
prediction_prob <- predict(rpartModel, val, type ="prob")
prediction_prob

# Vamos a ordenar la clase positiva YES (los que sobreviven)

rpartCtrl <- trainControl(verboseIter = T, classProbs = TRUE, summaryFunction = twoClassSummary, method = "cv", number = 10)
rpartModel <- train(Survived ~ .,
                    data = train,
                    method = "rpart",
                    metric = "ROC", # use la metrica de ROC y no Accuracy
                    trControl = rpartCtrl,
                    tuneGrid = rpartParametersGrid )

library(pROC)
predictionValidationProb <- predict(rpartModel, val, type ="prob")
auc <- roc(val$Survived, predictionValidationProb[["Yes"]], levels = unique(val[["Survived"]]))
roc_validation <- plot.roc(auc, ylim = c(0,1), type = "S", print.thres = T, main = paste("Validation AUC:", round(auc$auc[[1]], 2)))

```

En clases desiquilibradas, las curvas ROC también tienen sus limitaciones.

# Validación cruzada

Un procedimiento para resolver el problema de que un algoritmo es dependiente del orden en el que se presentan los datos es recurrir a la validación cruzada. A la hora de calcular el modelo de predicción, lo que se hace es volver a segmentar el conjunto de entrenamiento e ir probando con esos segmentos. El mejor modelo de ellos es el resultante. (ver trasparencia 18).

Caret maneja eso internamente. La selección del modelo se hace con el parámetro donde se indica la métrica en el trainControl. (method = "cv", number=10 --> numero de particiones)

# RandomForest

En el entrenamiento se especifica el method="rf". En el caso de pasar un grid, deberíamos de (en lugar de cf como es en árboles de regresión) indicar la rejilla de parámetros. Randomforest es más opaco a la hora de interpretar el modelo, es decir, no es como el árbol de regresión en el cuál podíamos obtener directamente las reglas, sino que en este caso no tiene una interpretación directa.

```{r}
rfModel <- train(Survived ~., data = train, method = "rf", metric = "ROC", trControl = rpartCtrl)

predictionValidationProb <- predict(rfModel, val, type ="prob")
auc <- roc(val$Survived, predictionValidationProb[["Yes"]], levels = unique(val[["Survived"]]))
roc_validation <- plot.roc(auc, ylim = c(0,1), type = "S", print.thres = T, main = paste("Validation AUC:", round(auc$auc[[1]], 2)))
```




